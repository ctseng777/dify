model: Meta-Llama-3.1-8B-Instruct  # Model identifier
# Model display name, can be set in en_US English and zh_Hans Chinese, zh_Hans will default to en_US if not set.
# Alternatively, if the label is not set, use the model identifier content.
label:
  en_US: Meta-Llama-3.1-8B-Instruct
model_type: llm  # Model type, claude-2.1 is an LLM
features:  # Supported features, agent-thought for Agent reasoning, vision for image understanding
- agent-thought
model_properties:  # Model properties
  mode: chat  # LLM mode, complete for text completion model, chat for dialogue model
  # TODO: Check with engineering
  context_size: 200000  # Maximum supported context size 
parameter_rules:  # Model invocation parameter rules, only required for LLM
- name: temperature  # Invocation parameter variable name
  # Default preset with 5 variable content configuration templates: temperature/top_p/max_tokens/presence_penalty/frequency_penalty
  # Directly set the template variable name in use_template, which will use the default configuration in entities.defaults.PARAMETER_RULE_TEMPLATE
  # If additional configuration parameters are set, they will override the default configuration
  use_template: temperature
  default: 0.7  # Default value for temperature
  min: 0.0  # Minimum value
  max: 1.0  # Maximum value
- name: top_p
  use_template: top_p
  default: 0.9  # Default value for top_p
  min: 0.0  # Minimum value
  max: 1.0  # Maximum value
- name: top_k
  label:  # Invocation parameter display name
    zh_Hans: Sampling quantity
    en_US: Top k
  type: int  # Parameter type, supports float/int/string/boolean
  help:  # Help information, describing the role of the parameter
    zh_Hans: Only sample from the top K options for each subsequent token.
    en_US: Only sample from the top K options for each subsequent token.
  required: false  # Whether required, can be left unset
  default: 50  # Default value for top_k
  min: 1  # Minimum value
  max: 100  # Maximum value
- name: max_tokens_to_sample
  use_template: max_tokens
  default: 4096  # Default parameter value
  min: 1  # Minimum parameter value, only applicable for float/int
  max: 4096  # Maximum parameter value, only applicable for float/int
- name: stop
  type: string  # Parameter type, supports list of strings #[Question] Sambanova allows a list of stop string, but the dify pydantic doesn't allow list
  help:  # Help information, describing the role of the parameter
    zh_Hans: 停止生成的標記。
    en_US: Tokens that signal the model to stop generating.
  required: false  # Whether required, can be left unset
  default: []  # Default value for stop tokens
- name: stream
  type: boolean  # Parameter type, supports boolean
  help:  # Help information, describing the role of the parameter
    zh_Hans: 是否流式返回生成的内容。
    en_US: Whether to stream the generated content.
  required: false  # Whether required, can be left unset
  default: false  # Default value for streaming


# TODO: Check with marketing
#pricing:  # Pricing information
#  input: '8.00'  # Input price, i.e., Prompt price
#  output: '24.00'  # Output price, i.e., returned content price
#  unit: '0.000001'  # Pricing unit, i.e., the above prices are per 100K
#  currency: USD  # Currency